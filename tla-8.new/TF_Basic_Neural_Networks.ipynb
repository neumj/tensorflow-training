{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pylib.draw_nn import draw_neural_net_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = None\n",
    "\n",
    "def reset_vars():\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def reset_tf():\n",
    "    global sess\n",
    "    if sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- requirement: pylib/__init__.py -->\n",
    "<!-- requirement: pylib/draw_nn.py -->\n",
    "<!-- requirement: images/neuron.svg -->\n",
    "\n",
    "# Basic Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operation of a logistic regression model is not so dissimilar from a neuron.  A neuron has many dendrites, each of which gets some input.  These inputs are individually weighted and all of the inputs are summed by the soma.  Depending on the result, the neuron may send a signal out its axon, to trigger other neurons.\n",
    "\n",
    "![neuron](images/neuron.svg)\n",
    "<!-- Copyright Quasar Jarosz.  Distributed under the CC Attribution-Share Alike 3.0 Unported Licence.  https://commons.wikimedia.org/wiki/File:Neuron_Hand-tuned.svg -->\n",
    "\n",
    "Similarly, a logistic regression takes many inputs, weights each, and combines them into a single output.  We can think of this model as an **artificial neuron**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_neural_net_fig([2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analogy should not be taken too seriously&mdash;real neurons fire a discrete pulse every time the potential in the soma exceeds a threshold, while our model outputs a continuous value depending on the current value of all its inputs.  Nonetheless, there is enough similarity to think that wiring together artificial neurons could improve their performance, in the same way that an entire brain performs better than a single neuron.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The XOR problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research into artificial neurons dates to the late 40s, but it was not until 1969 that Martin Minsky and Seymour Papert pointed out that basic neurons were unable to reproduce the **exclusive-or** (XOR) function.  This Boolean function of two Boolean variables returns true if exactly one of its inputs is true:\n",
    "\n",
    "$$ \\mathrm{XOR}(0, 0) = \\mathrm{XOR}(1, 1) = 0 \\ \\ \\ \\ \\ \\ \\mathrm{XOR}(0, 1) = \\mathrm{XOR}(1, 0) = 1 $$\n",
    "\n",
    "Below, we create a related two-class classification problem, with one class clustered about (0, 0) and (1, 1), and the other about (0, 1) and (1, 0).  It would be quite easy to draw a boundary separating the two classes by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = np.array([[0, 0]] * 100 + [[1, 1]] * 100\n",
    "                   + [[0, 1]] * 100 + [[1, 0]] * 100)\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(0, 0.2, (400, 2)) + centers\n",
    "labels = np.array([[0]] * 200 + [[1]] * 200)\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1], c=labels.ravel(), cmap=plt.cm.RdYlBu)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we build a simple logistic classifier for these data.  It takes two features of input and returns a prediction for the probability of being in class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 2], name=\"features\")\n",
    "y_label = tf.placeholder(tf.float32, [None, 1], name=\"labels\")\n",
    "\n",
    "W = tf.Variable(tf.zeros([2, 1]), name=\"weights\")\n",
    "b = tf.Variable(tf.zeros([1]), name=\"biases\")\n",
    "y = tf.matmul(x, W) + b\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y, \n",
    "                                                              labels=y_label))\n",
    "train = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "predicted = tf.cast(tf.nn.sigmoid(y) > 0.5, np.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, y_label), np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train it for a little.  (We might get slightly better performance with minibatches, but there's little enough data that the full batch performs okay.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_vars()\n",
    "\n",
    "for i in range(300):\n",
    "    sess.run(train, feed_dict={x: data, y_label: labels})\n",
    "    if i % 30 == 0:\n",
    "        print(sess.run([loss, accuracy],\n",
    "                       feed_dict={x: data, y_label: labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a bit odd.  The model seems to get stuck on 52% accuracy.  More notably, the entropy has barely improved at all.\n",
    "\n",
    "Let's visualize the predictions.  Note that the color scale only covers a portion of the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = np.column_stack(a.reshape(-1) for a in np.meshgrid(np.r_[-1:2:100j], np.r_[-1:2:100j]))\n",
    "ymesh = sess.run(tf.nn.sigmoid(y), feed_dict={x: mesh})\n",
    "ww, bb = sess.run([W, b])\n",
    "\n",
    "plt.imshow(ymesh.reshape(100,100), cmap=plt.cm.RdYlBu, origin='lower',\n",
    "           extent=(-1, 2, -1, 2), vmin=0.45, vmax=0.55)\n",
    "plt.colorbar()\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels.ravel(), cmap=plt.cm.RdYlBu,\n",
    "            edgecolor='w', lw=1)\n",
    "xx = np.linspace(-1, 2, 100)\n",
    "yy = -ww[0] / ww[1] * xx - bb / ww[1]\n",
    "plt.plot(xx, yy, 'k--')\n",
    "plt.axis((-1, 2, -1, 2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the problem becomes apparent: The model is returning nearly a 50% probability for each observation.  Therefore the entropy is stuck at just about $-\\log\\frac12 \\approx 0.6931$.\n",
    "\n",
    "We can draw a line at $p = 0.5$ to separate the two classes.  From the logistic function, we know this is equivalent to $x\\cdot W + b = 0$.  Thus, our model is attempting to draw a straight line through the plane to separate the two classes.  (In the general case, logistic regression separates the classes with a $(n-1)$-D hyperplane in $n$-D space.)  No line can do that in this case, so the model falls back to guessing 50% for each.\n",
    "\n",
    "We might wonder why the line wasn't chosen to separate one cluster from the other three.  This could have given us an accuracy approaching 0.75.  But remember that we are optimizing entropy, which is based on the probability estimates, not accuracy.  Because the probability grows the further we go from the threshold line, the penalty for the one cluster on the wrong side of the line would outweigh the gains from the two clusters put fully on the right side.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to combine these artificial neurons into a more complex configuration.  We'll make a network with a single **hidden layer** of size two.  That is, we will have two logistic regressions whose outputs are not visible.  Instead, they are fed into a third, visible neuron, whose output we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_neural_net_fig([2, 2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The math behind this isn't as bad as it might seem at first.  All of the weights of the neurons in the hidden layer can be combined into a single $2\\times2$ matrix $W^{(1)}$.  The final neuron's weights will be in a $2\\times1$ matrix $W^{(2)}$.  The biases behave similarly.  Then our final probabilistic prediction is just\n",
    "\n",
    "$$ p_j = f_2\\bigg( f_1\\left( X_{ji} W^{(1)}_{ik} + b^{(1)}_k \\right) W^{(2)}_k + b^{(2)} \\bigg)$$\n",
    "\n",
    "We are using the Einstein notation: All repeated indices are implicitly summed over.  Both $f_1$ and $f_2$ represent the logistic function, which is taken to operate element-wise over tensors.\n",
    "\n",
    "The **backpropagation** algorithm, developed by Paul Werbos in 1975, points out that we can use gradient descent (or similar algorithms) to optimize all of the parameters in these sorts of expressions.  All it takes is successive applications of the chain rule.  In fact, there's nothing special we have to do to make use of it: TensorFlow's optimizers automatically work though the successive derivatives to generate the update rules.  All we have to do is set up the calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 2\n",
    "W1 = tf.Variable(tf.random_normal([2, hidden_size], seed=42), name=\"weight1\")\n",
    "b1 = tf.Variable(tf.zeros([hidden_size]), name=\"bias1\")\n",
    "\n",
    "hidden = tf.nn.sigmoid(tf.matmul(x, W1) + b1, name=\"hidden\")\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([hidden_size, 1], seed=24), name=\"weight2\")\n",
    "b2 = tf.Variable(tf.zeros([1]), name=\"bias2\")\n",
    "\n",
    "y = tf.matmul(hidden, W2) + b2\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y,\n",
    "                                                              labels=y_label))\n",
    "train = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "predicted = tf.cast(tf.nn.sigmoid(y) > 0.5, np.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, y_label), np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's run it.  We need a few more steps to get all of the weights well-trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_vars()\n",
    "\n",
    "for i in range(3000):\n",
    "    sess.run(train, feed_dict={x: data, y_label: labels})\n",
    "    if i % 300 == 0:\n",
    "        print(sess.run([loss, accuracy],\n",
    "                       feed_dict={x: data, y_label: labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify the improved accuracy by examining our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = np.column_stack(a.reshape(-1) for a in np.meshgrid(np.r_[-1:2:100j], np.r_[-1:2:100j]))\n",
    "ymesh = sess.run(tf.nn.sigmoid(y), feed_dict={x: mesh})\n",
    "\n",
    "plt.imshow(ymesh.reshape(100,100), cmap=plt.cm.RdYlBu, origin='lower',\n",
    "           extent=(-1, 2, -1, 2), vmin=0, vmax=1)\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels.ravel(), cmap=plt.cm.RdYlBu,\n",
    "            edgecolor='w', lw=1)\n",
    "plt.axis((-1, 2, -1, 2))\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is that stripe created?  We can get some understanding by looking at the weights of the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww1 = sess.run(W1)\n",
    "hmesh = sess.run(hidden, feed_dict={x: mesh})\n",
    "\n",
    "for i in range(hidden_size):\n",
    "    plt.subplot(1, hidden_size, i+1)\n",
    "    plt.imshow(hmesh[:, i].reshape((100, 100)), origin='lower', cmap=plt.cm.RdYlBu,\n",
    "               extent=(-0.5,1.5,-0.5,1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see how each hidden neuron is just defining a line through the feature space.  Each line defines one side of the strip.\n",
    "\n",
    "Note that the colors are inverted from the final probabilities.  We can understand what's going on by examining the weights in the second layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Number of hidden neurons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the number of neurons in the hidden layer.  How does this change the predictions made by the model?  What happens when you add many neurons to this hidden layer?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How important is the logistic function in the hidden neurons?  We can easily take them out and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 2\n",
    "W1 = tf.Variable(tf.random_normal([2, hidden_size]), name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([hidden_size]), name=\"bias1\")\n",
    "\n",
    "hidden = tf.matmul(x, W1) + b1\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([hidden_size, 1]), name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([1]), name=\"bias2\")\n",
    "\n",
    "y = tf.matmul(hidden, W2) + b2\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y, \n",
    "                                                              labels=y_label))\n",
    "train = tf.train.GradientDescentOptimizer(1e-1).minimize(loss)\n",
    "\n",
    "predicted = tf.cast(tf.nn.sigmoid(y) > 0.5, np.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, y_label), np.float32))\n",
    "\n",
    "reset_vars()\n",
    "\n",
    "for i in range(3000):\n",
    "    sess.run(train,\n",
    "             feed_dict={x: data, y_label: labels})\n",
    "    if i % 300 == 0:\n",
    "        print(sess.run([loss, accuracy], \n",
    "                       feed_dict={x: data, y_label: labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look good.  It seems that we've fallen back to the single neuron case again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = np.column_stack(a.reshape(-1) for a in np.meshgrid(np.r_[-1:2:100j], np.r_[-1:2:100j]))\n",
    "ymesh = sess.run(tf.nn.sigmoid(y), feed_dict={x: mesh})\n",
    "\n",
    "plt.imshow(ymesh.reshape(100,100), cmap=plt.cm.RdYlBu, origin='lower',\n",
    "           extent=(-1, 2, -1, 2), vmin=0.45, vmax=0.55)\n",
    "plt.colorbar()\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels.ravel(), cmap=plt.cm.RdYlBu,\n",
    "            edgecolor='w', lw=1)\n",
    "plt.axis((-1, 2, -1, 2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this behavior becomes obvious when we consider the full neural network function, with $f_1$ being the identity function:\n",
    "\n",
    "$$ p_j = f_2\\bigg( \\left( X_{ji} W^{(1)}_{ik} + b^{(1)}_k \\right) W^{(2)}_k + b^{(2)} \\bigg) = f_2\\bigg( X_{ji} \\color{red}{W^{(1)}_{ik} W^{(2)}_k} + \\color{blue}{b^{(1)}_k W^{(2)}_k + b^{(2)}} \\bigg) $$\n",
    "\n",
    "This is just logistic regression, with the <font color=\"red\">weights</font> and <font color=\"blue\">bias</font> written in a rather funny way.  Given this, it would be odd if we didn't see this behavior!\n",
    "\n",
    "This function, $f_1$, is known as the **activation function** of the neuron.  As we just saw, the fact that the activation function is nonlinear is crucial.  This is what keeps the whole network from just being a linear transformation.  Any non-linearity will do though, so a number of different activation functions have been proposed.  Here are a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(-4, 4)\n",
    "plt.plot(xx, xx > 0, label='Heaviside')\n",
    "plt.plot(xx, sess.run(tf.nn.sigmoid(xx)), label='sigmoid')\n",
    "plt.plot(xx, sess.run(tf.nn.tanh(xx)), label='tanh')\n",
    "plt.plot(xx, sess.run(tf.nn.relu(xx)), label='relu')\n",
    "plt.legend(loc=2)\n",
    "plt.ylim(-1, 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first perceptron, a single-layer neural network, designed by Frank Rosenblatt in 1957, used the **Heaviside** or **step** function.  This is essentially equivalent to using a threshold with logistic regression.  While this is fine for predicting a class, it has slope 0 almost everywhere, and therefore is unsuitable for use with gradient descent algorithms.\n",
    "\n",
    "We have already seen the **sigmoid** function used in logistic regression.  In a sense, it smooths out the step function, allowing a usable gradient in the area near $x = 0$.  Because the function saturates at $\\pm\\infty$, the gradient goes to zero for large positive or negative inputs.  This can cause optimization algorithms to slow down.\n",
    "\n",
    "The average output of a sigmoid is 0.5, but it performs best when the average input is 0.  Thus, several layers of sigmoid neurons may push themselves away from optimal behavior.  One solution to this is use a **tanh** instead.  While the general shape is the same, its range is [-1, 1], so the output will on average be 0.\n",
    "\n",
    "The tanh will still have trouble with saturation of the signal.  Recently, many researchers have had success with the **rectified linear unit (ReLU)**: $f(x) = \\max(0, x)$.  While it might seem to combine the problems of the other functions (non-analytic points, zero derivatives, non-centered output), in practice it tends to be quite successful.\n",
    "\n",
    "ReLU neurons are susceptible to dying, however.  If they get into a state where the combined input is negative, both their output value and gradient become zero, and the neurons cease to learn.  Two other activation functions take the linear part of the ReLU and replace the constant zero.  The **leaky ReLU** is also linear for negative inputs, but with a much smaller slope, typically 0.01.  This keeps the benefits of the ReLU, but allows dead neurons to recover eventually.  The **exponential linear unit (ELU)** marries the linear portion to an exponential decay with negative inputs.  This allows the activation function to have a continuous derivative, at the cost of being more expensive to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x):\n",
    "    return tf.maximum(0.01 * x, x)\n",
    "\n",
    "plt.plot(xx, sess.run(tf.nn.relu(xx)), label='relu')\n",
    "plt.plot(xx, sess.run(leaky_relu(xx)), label='leaky relu', ls='--')\n",
    "plt.plot(xx, sess.run(tf.nn.elu(xx)), label='elu', ls=':')\n",
    "plt.legend(loc=2)\n",
    "plt.ylim(-1, 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sharp-eyed readers may have already noticed something done differently in the neural networks: the weights are initialized randomly, instead of being set to zero.  This breaks the symmetry of the two hidden neurons, so that they can evolve to detect different features.  All the weights being zero is a meta-stable state of the network, and gradient descent will not move it from that state.  A \"proper\" solution would be to set the weights to be orthogonal vectors, but in practice random weights are \"orthogonal enough\".\n",
    "\n",
    "Usually the biases can be initialized to zero.  A small positive bias is sometimes used with the ReLU activation function, to push the system towards non-zero gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Exploring activation functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the activation function in the XOR neural network.  What function gives the fastest training?  The highest accuracy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Adding a hidden layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the softmax classifier for the MNIST handwriting data set that we build in the previous notebook.  Add a hidden layer.  Adjust the size of that hidden layer and its activation function to optimize the performance of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2017 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
