{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- requirement: images/noise_0.png -->\n",
    "<!-- requirement: images/noisy_image_0.png -->\n",
    "<!-- requirement: images/nn-fool0.jpg -->\n",
    "<!-- requirement: images/nn-fool1.jpg -->\n",
    "<!-- requirement: images/negative1.png -->\n",
    "<!-- requirement: images/negative2.png -->\n",
    "<!-- requirement: images/subliminal-graffiti-sticker.jpg -->\n",
    "<!-- requirement: pylib/mnist_dataset.py -->\n",
    "<!-- requirement: pylib/tf_utils.py -->\n",
    "\n",
    "# Adversarial Noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fooling Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks are inspired by our own brains' wiring, but how close is their actual operation?  One way to judge is by looking at how they fail.  If we build an image to fool a neural network, is it also one that would fool a human?\n",
    "\n",
    "The answer, at present, is a resounding \"No\".  Consider these images, generated by Nguyen, Yosinski, and Clune in a [recent paper](https://arxiv.org/abs/1412.1897).  They trained a neural network for image recognition, and then build images that the network would classify with extreme confidence.  To us, the images appear to be noise.\n",
    "\n",
    "![fool0](images/nn-fool0.jpg)\n",
    "*A neural network classifies each of these images into the class below it with confidence $\\ge$ 99.6%.  From Nguyen A, Yosinski J, Clune J. Deep Neural Networks are Easily Fooled: High Confidence Predictions\n",
    "for Unrecognizable Images. In Computer Vision and Pattern Recognition (CVPR ’15), IEEE, 2015.*\n",
    "\n",
    "Perhaps this is not entirely surprising.  The net must classify the image as something, and it was not trained to recognize \"noise\".  Some combination of pixels ought to be able to tickle the right inputs in a way to produce a high-confidence classification.  More surprising, perhaps, are the following images, which produce equally confident classifications, despite having clear patterns and little resemblance to the objects in question.\n",
    "\n",
    "![fool1](images/nn-fool1.jpg)\n",
    "*A neural network classifies each of these images into the class below it with confidence $\\ge$ 99.6%.  From Nguyen A, Yosinski J, Clune J. Deep \"Neural Networks are Easily Fooled: High Confidence Predictions\n",
    "for Unrecognizable Images\". In Computer Vision and Pattern Recognition (CVPR ’15), IEEE, 2015.*\n",
    "\n",
    "This suggests that the features that a neural network is triggering on are in fact significantly different from those that our brains are picking out.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attacking Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results, though interesting, may seem nothing more than an intellectual curiosity.  The images are clearly artificial and can easily be picked out by a human.  However, it would be more worrying if it were possible to produce images that a human would confidently classify in one category, while a neural net work confidently classify it in another.  Such an image could be used to attack a system involving a classifier without being obvious.\n",
    "\n",
    "Researchers have managed to produce such attacks.  Starting with an arbitrary image, it is possible to make small, barely noticeable modifications that cause a neural network to change its classification of the image.  The follow examples come from a [paper](https://arxiv.org/abs/1312.6199) by Szegedy, *et al*.\n",
    "\n",
    "![negative1](images/negative1.png)\n",
    "![negative2](images/negative2.png)\n",
    "\n",
    "*On the left, sample images correctly classified by [`AlexNet`](https://en.wikipedia.org/wiki/AlexNet).  On the right, distorted images that `AlexNet` classifies as ostriches.  The center images show the differences between the original an modified images, magnified by a factor of 10. From C. Szegedy, W. Zaremba, I. Sytskever, J. Bruna, D. Erhan, I. Goodfellow, R. Fergus, \"Intriguing Properties of Neural Networks\".  `arXiv`:1312.6199, February 2014.*\n",
    "\n",
    "In this case, a unique noise was created for each input, but even that is unnecessary.  It is in fact possible to generate a single **adversarial noise** which can cause a neural network to misclassify most input images into whatever class the attacker desires.\n",
    "\n",
    "In this notebook, we will build an attack against a CNN designed to classify MNIST images.  It will produce a noise that looks like the left image, where red and blue pixels represent positive and negative changes to the pixel intensities.  When added to an MNIST image (right), this noise causes the CNN to classify the image as a zero.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"files/images/noise_0.png\" style=\"width: 400px;\"/> </td>\n",
    "        <td> <img src=\"files/images/noisy_image_0.png\" style=\"width: 400px;\"/> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "While adversarial noise is trained against a particular network, it is somewhat robust.  Noise trained on one network has been show to be able to attack a second network with the same architecture but trained independently, albeit with reduced efficiency.\n",
    "\n",
    "Such attacks are even possible in the real world.  [Recent work](https://iotsecurity.eecs.umich.edu/#roadsigns) by Evtimov, *et al*, have produced adversarial noise in the form of stickers applied to street signs.  These can reliably cause the misclassification of those signs by a CNN.\n",
    "\n",
    "![street sign](images/subliminal-graffiti-sticker.jpg)\n",
    "*A CNN misclassifies this as a \"Speed Limit 45\" sign in two-thirds of trials.  From I. Evtimov, K. Eykholt, E. Fernandes, T. Kohno, B. Li, A. Prakash, A. Rahmati, D. Song, \"Robust Physical-World Attacks on Machine Learning Models\". `arXiv`:1707.08945, August 2017.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do you find adversarial noise?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before attempting to correct for the noise, we first have to find it. We'll get different noise patterns for each class (0-9), so we'll have to calculate adversarial noise 10 times. We will leave this as an exercise for you, but will describe the process: \n",
    "\n",
    "1. Change all of the test class labels to a single class (the \"adversarial target class\"). \n",
    "2. Create a new loss function that is the sum of the original loss and the L2-norm loss (least squares error). \n",
    "3. Define an optimizer to minimize this loss (like gradient descent) where you change the adversarial noise to increase the number of images classified as the adversarial target class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two different optimization procedures in our neural net. The first is the typical procedure where we try to classify the digits and modify the weights and biases of our neural network. The second is trying to find the adversarial noise and is described above. In this procedure, we do not modify the variables of the neural network. \n",
    "\n",
    "To make the network immune to noise we have to train it twice. One time to find the noise and a second time to train network to correctly classify noisy images. In the example below, we do this for target class 3. (3 is similar to many of the other digits, so we can train in fewer steps.) In theory, we would like to do this for all classes, but there are several things to keep in mind:\n",
    "\n",
    "1. The model's accuracy decreases if we try to make it immune to all classes (0-9). \n",
    "2. If we have many classes, making the network immune to all adversarial noise is impractical. \n",
    "3. As the model becomes immune to noise, it does not classify clean images as accurately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the data and setting up the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylib.tf_utils import mnist_test, mnist_train\n",
    "from tensorflow.keras.utils import to_categorical as one_hot\n",
    "\n",
    "X_train, y_train = mnist_train()\n",
    "X_test, y_test = mnist_test()\n",
    "\n",
    "y_train = one_hot(y_train)\n",
    "y_test  = one_hot(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll add noise using a custom Keras layer that comes before the main network. The kernel of this layer (the noise) will be zero to start, and we'll make this layer non-trainable for the initial training of the network. In order to ensure that our noisy images remain recognizable to the human eye, we will also limit the magnitude of this noise. Here, we choose this limit to be 0.35. Finally, we want to ensure that the values of our pixels remain between 0 and 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "noise_limit = 0.35\n",
    "\n",
    "class NoiseLayer(Layer):\n",
    "    \n",
    "    def __init__(self, kernel_regularizer=None, noise_limit=noise_limit, **kwargs):\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.noise_limit = noise_limit\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=input_shape[1:],\n",
    "                                      initializer='zeros')  \n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):                        \n",
    "        self.kernel = K.clip(self.kernel, -self.noise_limit, self.noise_limit)\n",
    "        return K.clip(x + self.kernel, 0, 1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main network consists of two convolutional layers followed by a dense hidden layer and the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 10\n",
    "\n",
    "filt_size = [5, 5]\n",
    "img_size = 28\n",
    "out_sizes = [32, 64, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(NoiseLayer(kernel_regularizer=regularizers.l2(0.0000025)))\n",
    "model.add(keras.layers.Reshape([img_size, img_size, 1]))\n",
    "\n",
    "for out_size in out_sizes[:-1]:\n",
    "    model.add(keras.layers.Conv2D(out_size, filt_size, padding='same',\n",
    "                                  activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2),\n",
    "                                        padding='same'))\n",
    "    \n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(out_sizes[-1], activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(N_CLASSES, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we write a function that can implement two different modes of training:\n",
    "\n",
    "* `main`: &nbsp; When training the main network, all layers except the first (the noise layer) are trainable, and we use the usual targets.\n",
    "* `noise`: When training the noise, only the first layer is trainable, and we change all targets to equal the adversarial target class.\n",
    "\n",
    "We don't use the usual targets when training the noise, because we aren't trying to make correct predictions at this stage. Instead, we are trying to 'hack' the network to always predict `adversary_target_class`.\n",
    "\n",
    "Note that calling `model.compile` more than once does _not_ reset the weights of the network. Also note that the noise layer includes some mild regularization. This does not affect `main` training, but it helps to limit the kernel that will be learned in `noise` training. We want to learn the smallest values of noise which will result in the best (mis)classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mode, model, X_train, y_train, X_test, y_test, adversary_target_class=0, epochs=1):\n",
    "    \n",
    "    # Set the appropriate layers as trainable\n",
    "    for n, layer in enumerate(model.layers):\n",
    "        if n == 0:\n",
    "            if mode == \"main\":\n",
    "                layer.trainable = False\n",
    "            else:\n",
    "                layer.trainable = True\n",
    "        else:\n",
    "            if mode == \"main\":\n",
    "                layer.trainable = True\n",
    "            else:\n",
    "                layer.trainable = False\n",
    "    \n",
    "    # Set the appropriate targets \n",
    "    if mode == \"noise\":\n",
    "        target_train = np.ones(y_train.shape[0]) * adversary_target_class\n",
    "        target_test  = np.ones(y_test.shape[0]) * adversary_target_class\n",
    "        target_train = one_hot(target_train, 10)\n",
    "        target_test  = one_hot(target_test, 10)\n",
    "    else:\n",
    "        target_train = y_train\n",
    "        target_test  = y_test\n",
    "            \n",
    "    # Compile and train\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=keras.optimizers.Adam(), \n",
    "                  metrics=['accuracy'])            \n",
    "                                               \n",
    "    history = model.fit(X_train, target_train,            \n",
    "                    epochs=epochs,                 \n",
    "                    batch_size=100,\n",
    "                    validation_data=(X_test, target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we train the main part of the network to classify digits without noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\"main\", model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The noise term was initialized to zero.  (This is why it didn't disrupt the training above.)  We'll get slightly better performance if we start it off with some random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "noise_init = np.random.uniform(-noise_limit/2, noise_limit/2, size=(28*28,))\n",
    "model.layers[0].set_weights([noise_init])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This random noise doesn't particularly bother the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(idx):\n",
    "    image = X_test[idx]\n",
    "    return np.argmax(model.predict([[image]])[0])\n",
    "\n",
    "idx = 0\n",
    "actual = np.argmax(y_test[idx])\n",
    "print (\"Predicted: %d, Actual: %d\" % (predict(idx), actual))\n",
    "plt.imshow((X_test[idx]+noise_init).reshape((img_size,img_size)),\n",
    "           cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But by training with `adversary_target_cls=3`, we can tune the noise to force classification of the images as threes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\"noise\", model, X_train, y_train, X_test, y_test, adversary_target_class=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The noise displays hints of a three, but it is mostly random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = model.layers[0].get_weights()[0]\n",
    "plt.imshow(noise.reshape((img_size,img_size)), interpolation='nearest',\n",
    "           cmap='seismic', vmin=-1.0, vmax=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it is combined with an image, our classifier is fooled.  But when we look at the image, it's still clearly a seven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "actual = np.argmax(y_test[idx])\n",
    "print (\"Predicted: %d, Actual: %d\" % (predict(idx), actual))\n",
    "\n",
    "plt.imshow((np.clip(X_test[idx]+noise,0,1)).reshape((img_size,img_size)),\n",
    "           cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing additional training in `main` mode, we immunize the classifier against this noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\"main\", model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the classifier works on the noisy image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "actual = np.argmax(y_test[idx])\n",
    "print (\"Predicted: %d, Actual: %d\" % (predict(idx), actual))\n",
    "\n",
    "plt.imshow((np.clip(X_test[idx]+noise,0,1)).reshape((img_size,img_size)),\n",
    "           cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Extending immunity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the network immune to all target classes. How does the accuracy of the model change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2018 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
