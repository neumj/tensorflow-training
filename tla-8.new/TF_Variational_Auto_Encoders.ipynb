{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pylib import mnist_dataset\n",
    "from pylib.tensorboardcmd import tensorboard_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: increase this to at least 10 for a useful run, 20 or more produces better results\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- requirement: images/VAE.png -->\n",
    "<!-- requirement: pylib/__init__.py -->\n",
    "<!-- requirement: pylib/tensorboardcmd.py -->\n",
    "<!-- requirement: pylib/mnist_dataset.py -->\n",
    "<!-- requirement: pylib/tf_utils.py -->\n",
    "\n",
    "# Variational Autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders are neural networks where the number of input and output neurons are the same. If our input neurons represent pixels in an image, the output of our autoencoder will ideally be the input image. Why would we want to create a model that simply reproduces our data? If we restrict the number of neurons in our hidden layers to be less than the number of input or output neurons, we force our model to learn sparse representations of the data. Therefore, autoencoders can be used for image compression and removing noise from images. \n",
    "\n",
    "![VAE](images/VAE.png)\n",
    "\n",
    "An autoencoder consists of two neural networks -- an **encoder** and **decoder**. The encoder takes in high dimensional data and generates low dimensional representations of that data. Then, the decoder takes the low dimensional representations and translates them back into the high dimensional input space. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoders (VAEs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Autoencoders (VAEs) differ from regular autoencoders, because they not only learn sparse representations of data but also generate new data. Consequently, VAEs are used to [create new images](https://openai.com/blog/generative-models/). For example, we can train a VAE on the MNIST data set and have it create an image of a handwritten \"5.\" \n",
    "\n",
    "How do VAEs generate new data? They do so by making smart assumptions about the distributions of these sparse data representations, or **latent vectors**. More generally, they belong to a class of models called generative models, which learn the joint probability distribution between the input ($x$) and output (or latent vectors, $z$). We can then use this information to come up with likely $(x,z)$ pairs. For example, once we learn the distribution corresponding to the sparse representation of a handwritten \"5,\" we can sample from this distribution to form new latent vectors for \"5.\" \n",
    "\n",
    "Due to this constraint on the distributions of $z$, VAEs require an additional component in their loss function that penalizes deviations from these distributions.   \n",
    "\n",
    "In this tutorial, we will build a simple VAE to recreate images in the MNIST data set. We will start off as usual by resetting our session, loading our data, setting our variables, and defining our weights and biases functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to our summary logs\n",
    "now = datetime.now()\n",
    "logs_path = now.strftime(\"%Y%m%d-%H%M%S\") + '/summaries'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "img_size = 28\n",
    "img_size_flat = img_size * img_size\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "batch_size = 256\n",
    "num_iterations =  2000\n",
    "display_step = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylib.tf_utils import mnist_test, mnist_train\n",
    "train_images, train_label = mnist_train()\n",
    "test_images, test_label = mnist_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder and decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in above, VAEs make assumptions about the distribution of latent variables. Therefore, we will now consider VAEs from a probability framework. \n",
    "\n",
    "Recall **Bayes' Theorem** that tells us:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "P(z \\mid x) &=& \\frac{P(x \\mid z) \\cdot P(z)}{P(x)} \\\\\n",
    "\\\\\n",
    "\\text{posterior distribution} &=& \\frac{\\text{likelihood} \\times \\text{prior}}{\\text{marginal likelihood}}\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "\n",
    "Say we have data $x$ and latent variables $z$. The encoder tries to approximate the posterior distribution $P(z \\mid x)$, or generate latent variables conditioned on the data. On the other hand, the decoder takes $z$ [sampled from $P(z \\mid x)$] and outputs parameters to the likelihood distribution $P(x \\mid z)$. These parameters are the weights and biases of the neural networks. \n",
    "\n",
    "Going back to the neural network framework, in the code below, our encoder and decoder are neural networks (of two layers each) that are mirror images of each other. We feed the output of the encoder directly into the decoder to make the full autoencoder.\n",
    "\n",
    "We want to allow the flexibility to change the number of layers and their size without much work, so we'll build the layers up with a for loop through a list of sizes.\n",
    "\n",
    "Note that we are using the functional interface for this.  This allows us to build two distinct models, the encoder and decoder, then build our full model by using the former as the input to the latter.  This way, when we train the big model, we train both the decoder and encoder without extra work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll build a network that goes 784 -> 256 -> 128 -> 256 -> 784\n",
    "layer_sizes = [256, 128]\n",
    "\n",
    "original = keras.layers.Input(shape=(img_size_flat,))\n",
    "\n",
    "#Build the forward part, 784 -> 256 -> 128\n",
    "layer = original\n",
    "for size in layer_sizes:\n",
    "    layer = keras.layers.Dense(size, activation='sigmoid')(layer)\n",
    "#This is our encoder  \n",
    "encoder_out = layer\n",
    "\n",
    "encoder = keras.models.Model(original, encoder_out)\n",
    "#encoder.summary()\n",
    "\n",
    "#The decoder will be the reverse, 128 -> 256 -> 784\n",
    "#We'll need to reverse our layers, \n",
    "#drop the first one (it's input now), and add the final shape\n",
    "encoded_input = keras.layers.Input(shape=layer_sizes[-1:])\n",
    "layer = encoded_input\n",
    "for size in layer_sizes[-2::-1] + [img_size_flat]:\n",
    "    layer = keras.layers.Dense(size, activation='sigmoid')(layer)\n",
    "decoder_out = layer\n",
    "\n",
    "decoder = keras.models.Model(encoded_input, decoder_out)\n",
    "#decoder.summary()\n",
    "    \n",
    "autoencoder_out = decoder(encoder(original))\n",
    "autoencoder = keras.models.Model(original, autoencoder_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL-divergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want our network to be accurate, but we also want the latent variables to approximate the posterior distribution. The amount of information lost when approximating $P(z \\mid x)$ is called the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullbackâ€“Leibler_divergence), and we will use it to construct our loss function. To make our lives simple, we will choose the posterior distribution to be a unit normal and [calculate](http://allisons.org/ll/MML/KL/Normal/) the divergence accordingly. \n",
    "\n",
    "Why do we want our latent variables to approximate a certain distribution? We want them to be useful, so we impose this constraint. We can think of this as a form of  of regularization where we lose some fidelity to ensure we are capturing only important features. In other words, we want to build a model that can generate images and not just memorize them. A nice explanation of choosing latent variables can be found [here](http://kvfrans.com/variational-autoencoders-explained/).\n",
    "\n",
    "We also want to minimize the loss due to inaccurate pixel values and must therefore create a component of the loss function that penalizes these errors. Our loss function would then be the sum of these two contributions.\n",
    "\n",
    "We won't be doing that here, as it slows down training quite a bit - our two goals, accurately reproducing the pixels and shaping our latent variable distributions, are in tension.  In addition, this requires information from both the center layer (the latent vectors) and the final layer, making the encoding of the loss more involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike last time, we will use the Adam Optimizer instead of the Gradient Descent Optimizer. This optimizer uses a decaying learning rate. \n",
    "\n",
    "As a reminder, we can interpret the learning rate as the size of the step we take down a gradient of our loss function. If the step size is too large, we may never get to the minimum. A large learning rate will manifest itself as noise in our loss curve that never converges to a minimum point. However, if we have a very small step size, our model may take a long time to run. Ideally, we want to take large steps at the start of the training process and small steps towards the end. The [Adam Optimizer](https://arxiv.org/pdf/1412.6980v8.pdf) changes the learning rate for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will train our model.  Again, we note that our input and output are the same - we're training a model that reproduces its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(train_images, train_images, epochs=num_epochs, \n",
    "                batch_size=60, validation_data=(test_images,test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will regenerate 10 of the MNIST images. We will display the original test images above the regenerated ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 10\n",
    "\n",
    "# Applying encode and decode over test set\n",
    "encode_decode = autoencoder.predict(test_images[:n_examples])\n",
    "\n",
    "# Compare original images with their reconstructions\n",
    "f, a = plt.subplots(2, n_examples, figsize=(20, 4))\n",
    "for i in range(n_examples):\n",
    "    a[0][i].imshow(np.reshape(test_images[i], img_shape))\n",
    "    a[1][i].imshow(np.reshape(encode_decode[i], img_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Noise removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our system now has the capacity to reproduce the original images, and thus has some sort of internal model of what they should look like.  We can take advantage of this by feeding it images that _almost_ look like what it's seen before - it will return its best guess as to what they should be, given what it's seen.  Since we've trained it on clean images, we expect it will return clean images.  So, if we feed it a noisy image, it should do its best to return something like it's seen before, often very successfully removing the noise.\n",
    "\n",
    "Here we'll take an image, add a uniform random number to a percentage of the pixels, then run it through the encode-decode pathway.  We'll also run the clean image through the pathway to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def uniform_noise(img, percent):\n",
    "    start = test_images[img].copy()\n",
    "    noisy = start.copy()\n",
    "    for i in range(len(test_images[img])):\n",
    "        if random.random() < percent:\n",
    "            noisy[i] += random.random()\n",
    "            noisy[i] = min(noisy[i],1)\n",
    "    f, a = plt.subplots(1, 4, figsize=(20, 4))\n",
    "    encode_decode = autoencoder.predict(np.array([start, noisy]))\n",
    "    a[0].imshow(np.reshape(start, img_shape))\n",
    "    a[0].set_title(\"Input\")\n",
    "    a[0].grid(False)\n",
    "    a[1].imshow(np.reshape(encode_decode[0], img_shape))\n",
    "    a[1].set_title(\"Encode-decode on input\")\n",
    "    a[1].grid(False)\n",
    "    a[2].imshow(np.reshape(noisy, img_shape))\n",
    "    a[2].set_title(\"Noisy\")\n",
    "    a[2].grid(False)\n",
    "    a[3].imshow(np.reshape(encode_decode[1], img_shape))\n",
    "    a[3].set_title(\"Encode-decode on noisy\")\n",
    "    a[3].grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've made a little function to do just that, just give it which test image (there are $10000$ of them) and what percentage of the pixels to add noise to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "interact(uniform_noise, img=(0,100), percent=(0,0.4,0.05));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating new images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we'd be able to take a random draw from a 128-dimensional Gaussian of unit variance, and this would give us a new image.  That's the \"variational\" aspect, and enforced by the KL-divergence.  This turns out not to work particularly well for the data here.\n",
    "\n",
    "Instead, we'll cheat.  We don't know the underlying 128-dimensional distribution of our data, but we can see what happens when we manipulate the compressed representation.  Let's try making a new 4 by making a linear combination of two old 4's.  \n",
    "\n",
    "We have created our model in two pieces, an encoder and a decoder, and stitched them together.  Now we'll use them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first4 = 4\n",
    "second4 = 210\n",
    "four1 = test_images[first4]\n",
    "four2 = test_images[second4]\n",
    "def plot_images(im1, im2):\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    ax[0].imshow(im1.reshape(28,28))\n",
    "    ax[1].imshow(im2.reshape(28,28))\n",
    "plot_images(four1, four2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the model does reconstructing them before we move forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_decode = autoencoder.predict(test_images[[first4, second4]])\n",
    "\n",
    "plot_images(encode_decode[0], encode_decode[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we get their encoded representations and linearly interpolate between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_im = encoder.predict(test_images[[first4, second4]])\n",
    "code1 = encode_im[0]\n",
    "code2 = encode_im[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 5\n",
    "new4 = decoder.predict(np.array([code1*(1-p) + code2*p for p in np.linspace(0,1,n_examples)]))\n",
    "f, a = plt.subplots(1, n_examples, figsize=(20, 6))\n",
    "for i in range(n_examples):\n",
    "    a[i].imshow(np.reshape(new4[i], img_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: New numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a few new numbers as combinations of the old ones (they don't have to be linear combinations of two of them, you could do any size set, but remember that each pixel must be between 0 and 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: More compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have chosen a hidden size of 128, which is about a seventh of the original size, and it does very well.  Try playing with this size and see what happens if you make it smaller.  We tried with 10 and got surprisingly good results.  Why might this be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2018 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
