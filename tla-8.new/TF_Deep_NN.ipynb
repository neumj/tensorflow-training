{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pylib.draw_nn import draw_neural_net_fig\n",
    "\n",
    "sess = None\n",
    "\n",
    "def reset_vars():\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def reset_tf():\n",
    "    global sess\n",
    "    if sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- requirement: pylib/__init__.py -->\n",
    "<!-- requirement: pylib/draw_nn.py -->\n",
    "<!-- requirement: pylib/mnist_dataset.py -->\n",
    "<!-- requirement: images/dnn_overfit.png -->\n",
    "<!-- requirement: images/dnn_reg.png -->\n",
    "<!-- requirement: images/dnn_dropout.png -->\n",
    "<!-- requirement: images/dnn_batch.png -->\n",
    "\n",
    "# Deep Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is deep learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is a branch of machine learning that tries to emulate the biological structure and function of the brain using artificial neural networks. These networks include: \n",
    "\n",
    "- Multilayer Perceptron Networks\n",
    "- Convolutional Neural Networks\n",
    "- Recurrent Neural Networks\n",
    "\n",
    "Additionally, these networks are hierarchical or multilayered, enabling them to model high-level abstractions in data. For this reason, deep learning is also called **hierarchical learning**. (Notice how there are two hidden layers in the figure of the multilayer perceptron network below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_neural_net_fig([20, 14, 12, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are benefits to using hierarchical models. In contrast to the performance of older machine learning algorithms, the performance of deep learning algorithms scales with the amount of data they are trained on -- the more data, the better the model. Consequently, deep learning algorithms typically outperform traditional ones. These models also have the ability to automatically extract features from data in a process called [feature learning](https://en.wikipedia.org/wiki/Feature_learning). This ability eliminates the need for a priori knowledge of the data to construct features, which is particularly useful when dealing with complex data such as images.  \n",
    "\n",
    "Deep learning has some pretty neat applications. Not only can we classify images with a high degree of accuracy, but we can also use deep learning algorithms to [generate captions](https://research.googleblog.com/2016/09/show-and-tell-image-captioning-open.html), [summarize](https://research.googleblog.com/2016/08/text-summarization-with-tensorflow.html) and [translate](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html) text, [generate audio](https://deepmind.com/blog/wavenet-generative-model-raw-audio/), and [produce art](https://github.com/lengstrom/fast-style-transfer/). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by replicating some of the code we used for our basic neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylib.tf_utils import mnist_train, mnist_test\n",
    "\n",
    "X_train, y_train = mnist_train()\n",
    "X_test, y_test = mnist_test()\n",
    "\n",
    "N_PIXELS= 28 * 28\n",
    "N_CLASSES = 10\n",
    "\n",
    "hidden_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Weights and Biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, we want to initialize our weights with random values to break symmetry between neurons in a hidden layer. Additionally, we want to choose small values to avoid the **gradient vanishing problem**, where the weighted sum of the inputs (plus a bias) fall on the flat portion of the sigmoid curve. What is the proper scale of the weights?  Most of our activation functions have their best response for inputs of $\\mathcal O(1)$.  If we have $m$ random inputs, each of $\\mathcal O(1)$, we expect their sum to scale as $\\sqrt m$.  Therefore, weights are often chosen randomly with a mean of zero and standard deviation of $1/\\sqrt m$.\n",
    "\n",
    "For very large layers, this gives rather small weights.  An alternative approach is to only provide $k < m$ non-zero weights when initializing neurons.  This scheme, known as **sparse initialization**, provides more diversity amongst the neurons at initialization.  It can, however, also produce very slow convergence as \"incorrect\" choices of non-zero weights have to be removed.\n",
    "\n",
    "In the code below, we initialize our weights by sampling from a truncated normal distribution, where any weights greater than 2 standard deviations from the mean is re-picked. We also initialize the biases to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializer(shape):\n",
    "    return tf.truncated_normal(shape, stddev=shape[0]**-0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Hidden Layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single layer neural network only works well on linearly separable data. By adding one more layer, we can solve most classification problems. The exercise at the end of the basic neural network notebook was to add a layer to our model to improve the accuracy of our predictions. We will present the solution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, N_PIXELS], name=\"pixels\")\n",
    "y_label = tf.placeholder(tf.int64, [None,], name=\"labels\")\n",
    "\n",
    "W1 = tf.Variable(initializer([N_PIXELS, hidden_size]), name=\"weights\")\n",
    "b1 = tf.Variable(tf.zeros([hidden_size]), name=\"biases\")\n",
    "\n",
    "hidden = tf.nn.sigmoid(tf.matmul(x, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(initializer([hidden_size, N_CLASSES]), name=\"weights2\")\n",
    "b2 = tf.Variable(tf.zeros([N_CLASSES]), name=\"biases2\")\n",
    "\n",
    "y = tf.matmul(hidden, W2) + b2\n",
    "\n",
    "softmax = tf.nn.softmax_cross_entropy_with_logits_v2\n",
    "loss = tf.reduce_mean(softmax(logits=y, \n",
    "                              labels=tf.one_hot(y_label, 10)))\n",
    "train = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y, 1), y_label), tf.float32))\n",
    "\n",
    "batch_size  = 30\n",
    "steps_total = 10000\n",
    "steps_print = 1000\n",
    "\n",
    "reset_vars()\n",
    "\n",
    "for i in range(steps_total):\n",
    "    j = np.random.choice(len(y_train), batch_size, replace=False)\n",
    "    x_batch, y_batch = X_train[j,:], y_train[j]\n",
    "    sess.run(train, feed_dict={x: x_batch, \n",
    "                               y_label: y_batch})\n",
    "    if i % steps_print == 0 or i == steps_total - 1:\n",
    "        l, a = sess.run([loss, accuracy], feed_dict={x: X_test,\n",
    "                                                     y_label: y_test})\n",
    "        print(\"Test:  %0.5f, %0.5f\" % (l, a))\n",
    "        l, a = sess.run([loss, accuracy], feed_dict={x: X_train,\n",
    "                                                     y_label: y_train})\n",
    "        print(\"Train: %0.5f, %0.5f\" % (l, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up all of this math is obviously going to get tedious as we increase the number of layers. To address this, TensorFlow provides a number of higher level APIs to make creating neural networks easier, including the [layer API](https://www.tensorflow.org/api_docs/python/tf/layers), the [Estimator API](https://www.tensorflow.org/guide/estimators), and [Keras](https://www.tensorflow.org/api_docs/python/tf/keras).\n",
    "\n",
    "[Keras](https://keras.io/) is actually a high-level API that allows you to write code that will run on top of TensorFlow, [Theano](https://github.com/Theano/Theano), or [CNTK](https://github.com/Microsoft/cntk).  While Keras started as a third-party solution, TensorFlow added first-class support for the Keras API, and Google is now touting Keras as the preferred high-level API for TensorFlow.\n",
    "\n",
    "Keras provides [layer functions](https://keras.io/layers/core/), which let us create individual layers with a single line. Let's recreate this two-layer network using Keras' [sequential API](https://keras.io/models/sequential/).  We start by creating a `Sequential` object, which acts as a container for the rest of the model. We don't need to explicitly add an input component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been using **dense** layers.  That is, each neuron is connected to all of the inputs to the layer.  First we add the hidden layer to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        hidden_size,\n",
    "        activation='sigmoid',\n",
    "        use_bias=True,  # The default\n",
    "        kernel_initializer=keras.initializers.TruncatedNormal(stddev=N_PIXELS**-0.5)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sets up a weight matrix (referred to as the **kernel**, for reasons to be discussed later) of `input_dim` $\\times$ `hidden_size`, multiplies it with the input, adds a bias term (since `use_bias=True`), and sends the result through the sigmoid activation function.  We use a truncated normal `initializer` for the weights as before.\n",
    "\n",
    "We use a second dense layer to produce the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        10,\n",
    "        activation='softmax',\n",
    "        kernel_initializer=keras.initializers.TruncatedNormal(stddev=hidden_size**-0.5)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output for each observation is 10 dimensional while each label is a single number. We dealt with this before by building one-hot encoding into our loss function. Here we'll manually one-hot encode the labels instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical as one_hot\n",
    "\n",
    "y_train = one_hot(y_train)\n",
    "y_test  = one_hot(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process is simpler to code in Keras since the lower level details are handled for us.  We simply specify the loss function, optimization algorithm, and reporting metrics that define the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(lr=0.5),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the `.fit` method to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=30,\n",
    "                    validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that successive calls of the `.fit` method will perform additional training steps without resetting any model parameters. (Like `.partial_fit` in Scikit-learn)   \n",
    "\n",
    "The output is a history object which contains the metric values reported during training. It's often helpful to plot these to visualize how training is progressing.  \n",
    "\n",
    "However, you should be aware that **Keras does not report training and validation metrics in the same way**. Validation metrics are computed once at the end of each epoch, but training metrics are computed once for each batch and then averaged over the epoch. This is similar in effect to applying a time lag (of $0.5$ epochs) to the training metrics. The resulting distortion can be severe at the beginning of training if the model is improving rapidly, but disappears as training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")\n",
    "\n",
    "plt.plot(history.history['loss'], label=\"Training\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the trained model can also be used to make predictions. The syntax is the same as Scikit-learn, but keep in mind that our model ends with a softmax layer. This means that we'll get ten dimensional output (with probabilities corresponding to each class) for each input observation instead of a single number. If we want to actually predict labels, then we need to look at which class has the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test[:3])\n",
    "\n",
    "print(\"Raw Predictions:\\n\")\n",
    "print(predictions,\"\\n\")\n",
    "\n",
    "print(\"Rounded Probabilities:\\n\")\n",
    "print(np.round(predictions,3),\"\\n\")\n",
    "\n",
    "print(\"Predicted Labels:\\n\")\n",
    "print(np.argmax(predictions, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This API makes it easy to add layers and neurons to neural network. However, in doing so, we run the risk of overfitting our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional API\n",
    "\n",
    "Although the sequential API has the advantage of simplicity, it is not well suited for complicated architectures involving branches. For this reason, Keras also has a [functional API](https://keras.io/getting-started/functional-api-guide/) with alternate syntax that is more similar to basic TensorFlow.\n",
    "\n",
    "One of the main differences is that we need to define the input layer explicitly. This does not necessarily need to be an `Input` layer (it could for example be a `Flatten` layer), but it does need to specify the shape of the input data.  Keras is smart enough to take that information and figure out the required shapes for the rest of the network.  Unlike TensorFlow, a Keras shape does not include the first axis (observation axis), so the shape that is passed into the first layer is the shape of a single observation.  For the specific purpose of Neural Networks, we almost always do not want to specify how many observations we will have in a particular batch of data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = keras.layers.Input(shape=(N_PIXELS,))\n",
    "\n",
    "# Compare to:\n",
    "# x = tf.placeholder(tf.float32, [None, N_PIXELS], name=\"pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers are created in a manner very similar to the sequential version.  These layers are then called with the input tensor as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = keras.layers.Dense(hidden_size,\n",
    "                            activation='sigmoid',\n",
    "                            kernel_initializer=keras.initializers.TruncatedNormal(stddev=N_PIXELS**-0.5)\n",
    "                           )(x)\n",
    "                            \n",
    "y = keras.layers.Dense(10,\n",
    "                       activation='softmax',\n",
    "                       kernel_initializer=keras.initializers.TruncatedNormal(stddev=hidden_size**-0.5)\n",
    "                      )(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last difference is that we need to create a model object that acts as a wrapper for the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Model(inputs=x, outputs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, the `.compile` and `.fit` methods work exactly as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(lr=0.5),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=30,\n",
    "                    validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "\n",
    "The fitting methods that Keras introduces accept [callbacks](https://keras.io/callbacks/) for introducing side effects during training. One of the most useful is the `EarlyStopping` callback, which can be set to monitor a metric (usually the validation loss) to terminate training when the metric stops improving. When combined with `ModelCheckpoint` callback, this can be used to easily retrieve models in the optimal state. There are also callbacks for logging, writing to TensorBoard, and adapting the learning rate during training (note some optimizers already feature adaptive learning rates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = keras.callbacks.ModelCheckpoint(filepath='./models/keras_example.hdf5',\n",
    "                                               monitor='val_loss',\n",
    "                                               verbose=1, save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,\n",
    "                                              verbose=1, min_delta=.01)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=30,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[checkpointer, earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2018 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
