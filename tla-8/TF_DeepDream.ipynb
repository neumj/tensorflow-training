{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import tarfile\n",
    "import zipfile\n",
    "\n",
    "# Image manipulation.\n",
    "import PIL.Image\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- requirement: images/chipmunk.jpg -->\n",
    "<!-- requirement: images/beach.jpg -->\n",
    "<!-- requirement: images/inception.svg -->\n",
    "\n",
    "# The Inception Model and the Deep Dream Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now, we have been dealing with rather shallow networks.  This is primarily due to the time it takes to train deep networks.  We can still play with deep networks, as long as someone else has done the work of training for us.\n",
    "\n",
    "A number of pre-trained models are available from the [TensorFlow models repository](https://github.com/tensorflow/models).  We will use one of these, an early version of the [Inception model](https://github.com/tensorflow/models/tree/master/research/inception), which does image classification.  We run some image classification ourselves, as well as use it to demonstrate the Deep Dream image manipulation algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to the introduction of the Inception model, the state of the art for image classification was convolutional neural nets.  Better performance was achieved by increase the depth of the network (the number of layers) as well as the width of the network (the number of channels in each layer).\n",
    "\n",
    "While successful, these larger networks have two major drawbacks.  First, the increased number of parameters makes the network prone to overfitting, requiring techniques like dropout and large training sets.  Second, this large size also requires significantly more computation resources for training.\n",
    "\n",
    "It has been known, from both theoretical analysis and biological analogs, that sparser networks should give better performance with fewer parameters.  However, these sparse networks are not a good fit for modern parallel computations: the gain in networks' performance would be washed out by the loss of computational performance in training.\n",
    "\n",
    "[Szegedy *et al*](https://arxiv.org/pdf/1409.4842v1.pdf) realized that they could mimic this sparsity out of a collection of dense elements.  They designed an **inception module** out of several small convolutional elements, and then stacked these blocks together.  (Yes, the name came from the movie.)  The particular network we're examining here is an early instance dubbed `GoogLeNet`.  It won one of the 2014 `ImageNet` challenges, beating the accuracy of previous winners while using $1/12$ as many parameters.  The architecture is sketched in below.  (We are neglecting two auxiliary classifiers used to help train the lower levels.)\n",
    "\n",
    "![inception](images/inception.svg)\n",
    "\n",
    "After some initial convolutional and normalization layers, a series of nine inception modules run in series.  The output is combined into a fully-connected layer and then a softmax classifier.\n",
    "\n",
    "The inception module uses several $1\\times1$ convolutional elements.  At first, this may seem to be useless.  After all, a $1\\times1$ kernel would just multiply each input pixel by the value of the kernel.  Remember that the kernel also extends across, and mixes, all of the channels of the image.  These kernels are generating new features by mixing the input features.  In these cases, there are fewer output channels than input channels, so these convolutions act as dimensional reductions.  (A [following paper](https://arxiv.org/pdf/1512.00567v3.pdf) by the same group suggests some slight modifications to this scheme.)\n",
    "\n",
    "All of the convolutions in the inception modules use the \"same\" padding.  This ensures that each of the four paths through the module have the same image dimensions, so the outputs can be stacked as separate channels.  The interplay between the dimensional reduction and the stacking is such that number of parameters in each inception module is roughly comparable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will download the trained `GoogLeNet` model and save it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"http://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip\"\n",
    "data_dir = os.path.expanduser(\"~/inception/5h/\")\n",
    "file_path = os.path.join(data_dir, 'inception5h.zip')\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    # Check if the download directory exists, otherwise create it.\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    # Download\n",
    "    with open(file_path, \"wb\") as local_file:\n",
    "        local_file.write(urlopen(data_url).read())\n",
    "    # Extract\n",
    "    zipfile.ZipFile(file_path, mode=\"r\").extractall(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is stored in Google's [Protocol Buffer](https://developers.google.com/protocol-buffers/) format.  TensorFlow makes it easy to load the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    path = os.path.join(data_dir, \"tensorflow_inception_graph.pb\")\n",
    "    with tf.gfile.FastGFile(path, 'rb') as file:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(file.read())\n",
    "        tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "session = tf.InteractiveSession(graph=graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph has been reconstructed, but we don't have Python references to any of the tensors.  Luckily for us, the creators of the graph gave the operations sensible names.  Therefore, we can get references to the tensors with the `get_tensor_by_name` method.  (The `:0` suffix indicates that these live on the first, and in this case only, computational device.)  We'll use this to get the input and final output of the net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_input = graph.get_tensor_by_name(\"input:0\")\n",
    "graph_output = graph.get_tensor_by_name(\"output2:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the graph will be the probability of the image belonging to each of about 1000 image classes from the `ImageNet` competition.  Those labels are stored in a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [s.strip() for s in \n",
    "             open(os.path.join(data_dir, 'imagenet_comp_graph_label_strings.txt')).readlines()]\n",
    "labels[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load up an image and try it out.  Here's a photograph the author took at Point Reyes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beach = PIL.Image.open('images/beach.jpg')\n",
    "beach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network is designed to work with $224\\times224$ pixel input images.  For the `ImageNet` competition, the team took 144 different crops of the image, ran each through the model, and then averaged the softmax probabilities.  We won't go through that much work; we'll just resize the image down to the input size and send it through.  (The call to `np.expand_dims` is necessary because the network expects a stack of images as input.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.uint8(beach.resize((224, 224)))\n",
    "res = session.run(graph_output, {graph_input: np.expand_dims(image, axis=0)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking up the label for the highest probability, we get our prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[res[0].argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!  We can also look at the top predictions, all of which seem fairly reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in res.argsort()[0][-1:-6:-1]:\n",
    "    print(labels[i], res[0,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it with another image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chipmunk = PIL.Image.open('images/chipmunk.jpg')\n",
    "chipmunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.uint8(chipmunk.resize((224, 224)))\n",
    "res = session.run(graph_output, {graph_input: np.expand_dims(image, axis=0)})\n",
    "\n",
    "for i in res.argsort()[0][-1:-6:-1]:\n",
    "    print(labels[i], res[0,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is somewhat confident that this is a wood rabbit (see training examples at `http://www.image-net.org/synset?wnid=n02325366`).  On the one hand, it's a bit of an odd misclassification; on the other, they aren't entirely dissimilar.\n",
    "\n",
    "This becomes more reasonable when you consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'chipmunk' in labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model wasn't trained on chipmunks, so it could not classify the image as one, nor could it learn the differences between chipmunks and rabbits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Deep Dream Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But simply classifying images is not the only thing we can do with an inception network.  We can use deep learning to [create artwork](https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html?m=1) as well. Using the [Deep Dream](https://en.wikipedia.org/wiki/DeepDream) algorithm, we can modify images by amplifying the features detected by neural networks. We calculate the gradients in the model and then update the image accordingly. \n",
    "\n",
    "To create our artwork we will do the following:\n",
    "\n",
    "1. Select a layer in the inception model\n",
    "2. Feed the model an image\n",
    "3. Calculate the gradient in the layer with respect to the image\n",
    "4. Update the gradient to the image\n",
    "5. `goto 3`\n",
    "\n",
    "We've selected 12 layers from this model that we can explore.  These layers are highlighted and named in the previous architectural diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get graph\n",
    "layer_names = ['conv2d0', 'conv2d1', 'conv2d2', 'mixed3a', \n",
    "               'mixed3b', 'mixed4a', 'mixed4b', 'mixed4c', \n",
    "               'mixed4d', 'mixed4e', 'mixed5a', 'mixed5b']\n",
    "\n",
    "layer_tensors = [graph.get_tensor_by_name(name + \":0\") for name in layer_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the Deep Dream algorithm is to make an image that creates the largest activation in some particular layer.  We don't care about the sign of this activation, so we try to maximize the mean squared activation value.\n",
    "\n",
    "TensorFlow's automatic differentiation allows us to take the gradient of this quantity with respect to each pixel in the input image.  By adding a term proportional to the gradient to the image, we should increase the overall activation.  By repeating this process, we allow the feedback to amplify interesting patterns in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(graph_input, layer_tensor, image,\n",
    "             num_iterations=10, step_size=3.0):\n",
    "\n",
    "    # Convert image to np array\n",
    "    img = np.float32(image)\n",
    "\n",
    "    # Calculate gradient\n",
    "    activation = tf.reduce_mean(tf.square(layer_tensor))\n",
    "    gradient = tf.gradients(activation, graph_input)[0]\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # Calculate the value of the gradient.\n",
    "        grad = session.run(gradient,\n",
    "                           {graph_input: np.expand_dims(img, axis=0)})\n",
    "        grad /= (np.std(grad) + 1e-8)\n",
    "        step_size_scaled = step_size / (np.std(grad) + 1e-8)\n",
    "\n",
    "        # Update the image by following the gradient.\n",
    "        grad = grad.reshape(img.shape)\n",
    "        img += grad * step_size_scaled\n",
    "\n",
    "    return np.clip(img, 0.0, 255.0).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can choose an input image and target layer, and run the process for several steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg_result = optimize(graph_input, layer_tensors[5], chipmunk,\n",
    "                     num_iterations=5, step_size=10.0)\n",
    "\n",
    "display(PIL.Image.fromarray(mg_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the features amplified at the different levels, we can get an idea of the features each layer is trying to detect.  We see that the lower levels detect simple patterns.  These serve as the building blocks for more complicated patterns deeper into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in layer_tensors:\n",
    "    mg_result = optimize(graph_input, layer, chipmunk, num_iterations=5, step_size=10.0)\n",
    "    print(layer.name)\n",
    "    display(PIL.Image.fromarray(mg_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First play around with the layer number. The higher numbers correspond to more complex shapes that the model recognizes. \n",
    "2. Upload your own picture and perform the DeepDream algorithm on it. \n",
    "3. Don't square the tensor before you calculate the gradient. How does this change your results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2017 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
